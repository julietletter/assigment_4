{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLDwWQqTWtqk"
      },
      "source": [
        "# Create Tranformed Dataset\n",
        "\n",
        "In this workbook, you will read in the Hospital Inpatient file. You are welcome to use DataFrame and/or SparkSQL API as you desire as long as it produces the expected results.\n",
        "\n",
        "## Instructions:\n",
        "1. Download the data from the website\n",
        "2. Save the raw data to S3 for future use\n",
        "3. Read the data into spark from S3. There should be 34 columns.\n",
        "4. Investigate each column of the dataset\n",
        "5. Identify which columns you think are most benificial to the modeling task\n",
        "6. Determine how you might want to transform any columns to be more amenable to logistic regression. For instance normalizing certain data fields. \n",
        "7. Save your transformed dataset to s3. \n",
        "\n",
        "You are welcome to add as many cells as you need below up until the next section. **You must include comments in your code**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1E76Uqk1brtd"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession as \n",
        "from pyspark.sql.functions import monotonically_increasing_id\n",
        "import findspark\n",
        "findspark.init()\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "spark\n",
        "NY_discharge=spark.read.csv(\"./assigment 4/Hospital_Inpatient_Discharges__SPARCS_De-Identified___2015.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_p5Nw5Ib3K4"
      },
      "source": [
        "## In the following cells, please provide the requested code and output. Do not change the order and/or structure of the cells"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgmVpXLvb-yq"
      },
      "source": [
        "In the next cell, provide the code that saves your transformed dataset to your S3 bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLn4jAM9b2Uv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gi-RNAYfcDsH"
      },
      "source": [
        "In the next cell, print the schema of your transformed dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ARWWomBcHt2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXvvgjDRcHSG"
      },
      "source": [
        "In the next cell, print the first 10 records of your transformed dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNh57Qn4cK30"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pftK33GcLbA"
      },
      "source": [
        "In the next cell, print the row count of your transformed dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORC7M6ekcNcs"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "transform-data.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}